# å¤šæ¨¡æ€æ—¶åºQwen3æ¨¡å‹

åŸºäºLLaVAèŒƒå¼çš„æ—¶é—´åºåˆ—+æ–‡æœ¬å¤šæ¨¡æ€æ¨¡å‹ï¼Œç»“åˆPatchTSTæ—¶åºç¼–ç å™¨å’ŒQwen3è¯­è¨€æ¨¡å‹ã€‚

## ğŸ“‹ ç›®å½•

- [æ¶æ„æ¦‚è¿°](#æ¶æ„æ¦‚è¿°)
- [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
- [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)
- [è®­ç»ƒæµç¨‹](#è®­ç»ƒæµç¨‹)
- [æ¨ç†ä½¿ç”¨](#æ¨ç†ä½¿ç”¨)
- [é¡¹ç›®ç»“æ„](#é¡¹ç›®ç»“æ„)

## ğŸ—ï¸ æ¶æ„æ¦‚è¿°

```
æ—¶é—´åºåˆ— [n_vars Ã— seq_len]
    â†“
PatchTSTç¼–ç å™¨ [é¢„è®­ç»ƒ/å†»ç»“]
    â†“
Patchç‰¹å¾ [n_vars Ã— n_patches Ã— d_model]
    â†“
MLPæŠ•å½±å±‚ [å¯è®­ç»ƒ]
    â†“
æŠ•å½±ç‰¹å¾ [n_vars Ã— n_patches Ã— hidden_size]
    â†“
<ts> tokenæ›¿æ¢ â†’ èåˆEmbeddingåºåˆ—
    â†“
Qwen3æ¨¡å‹ [LoRAå¾®è°ƒ]
    â†“
æ–‡æœ¬ç”Ÿæˆ
```

### æ ¸å¿ƒç‰¹æ€§

- âœ… **ä¸¤é˜¶æ®µè®­ç»ƒ**ï¼šé¢„è®­ç»ƒæŠ•å½±å±‚ + LoRAå¾®è°ƒ
- âœ… **çµæ´»çš„æ—¶åºç¼–ç å™¨**ï¼šæ”¯æŒPatchTSTæƒé‡åŠ è½½å’Œå†»ç»“/è§£å†»
- âœ… **ç‰¹æ®Štokenè®¾è®¡**ï¼šä½¿ç”¨`<ts>`ä½œä¸ºæ—¶åºå ä½ç¬¦ï¼ˆç±»ä¼¼LLaVAçš„`<image>`ï¼‰
- âœ… **æ‰¹é‡å¤„ç†**ï¼šæ”¯æŒä¸åŒé•¿åº¦çš„æ—¶é—´åºåˆ—ï¼ˆé€æ ·æœ¬ç¼–ç ï¼‰
- âœ… **QLoRAæ”¯æŒ**ï¼š4-bité‡åŒ–ï¼Œé™ä½æ˜¾å­˜éœ€æ±‚

## ğŸ› ï¸ ç¯å¢ƒé…ç½®

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 2. æ£€æŸ¥ç¯å¢ƒ

```bash
python scripts/check_env.py
```

### 3. PatchTSTæƒé‡

ç¡®ä¿PatchTSTé¢„è®­ç»ƒæƒé‡å­˜åœ¨ï¼š
```bash
ls PatchTST_supervised/checkpoints/*/checkpoint.pth
```

## ğŸ“Š æ•°æ®å‡†å¤‡

### æ•°æ®æ ¼å¼ï¼ˆJSONLï¼‰

æ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼š

```json
{
  "input": "There are 2 time series. <ts></ts> and <ts></ts>. What patterns do you see?",
  "time_series": [
    [1.0, 2.0, 3.0, ..., 256ä¸ªå€¼],
    [4.0, 5.0, 6.0, ..., 256ä¸ªå€¼]
  ],
  "output": "Both time series show upward trends with seasonal patterns..."
}
```

### é‡è¦è§„åˆ™

1. âš ï¸ **`<ts></ts>`æˆå¯¹å‡ºç°**ï¼šåœ¨inputæ–‡æœ¬ä¸­ï¼Œæ¯ä¸ªå˜é‡å¯¹åº”ä¸€å¯¹`<ts></ts>`æ ‡ç­¾
2. âš ï¸ **æ•°é‡åŒ¹é…**ï¼š`<ts></ts>`çš„æ•°é‡å¿…é¡»ç­‰äº`time_series`æ•°ç»„é•¿åº¦
3. âš ï¸ **é•¿åº¦ä¸€è‡´**ï¼šåŒä¸€æ ·æœ¬å†…æ‰€æœ‰å˜é‡çš„é•¿åº¦ç›¸åŒï¼ˆé»˜è®¤256ï¼Œå¯é…ç½®ï¼‰
4. âš ï¸ **è·¨æ ·æœ¬å¯å˜**ï¼šä¸åŒæ ·æœ¬çš„å˜é‡æ•°å’Œé•¿åº¦å¯ä»¥ä¸åŒ

### ç¤ºä¾‹æ•°æ®å‡†å¤‡

```bash
# å‡è®¾ä½ çš„æ•°æ®åœ¨data/ç›®å½•ä¸‹
# data/pretrain.jsonl - é¢„è®­ç»ƒæ•°æ®ï¼ˆå¤§è§„æ¨¡ï¼‰
# data/finetune.jsonl - å¾®è°ƒæ•°æ®ï¼ˆä»»åŠ¡ç›¸å…³ï¼‰
```

## ğŸš€ è®­ç»ƒæµç¨‹

### é˜¶æ®µ1ï¼šé¢„è®­ç»ƒæŠ•å½±å±‚

åªè®­ç»ƒMLPæŠ•å½±å±‚ï¼ŒQwen3å’ŒPatchTSTéƒ½å†»ç»“ã€‚

```bash
# ä¿®æ”¹scripts/pretrain_projector.shä¸­çš„æ•°æ®è·¯å¾„
bash scripts/pretrain_projector.sh
```

**å…³é”®å‚æ•°**ï¼š
- `--tune_mm_mlp_adapter True`ï¼šåªè®­ç»ƒæŠ•å½±å±‚
- `--freeze_patchtst True`ï¼šå†»ç»“PatchTST
- `--learning_rate 1e-3`ï¼šè¾ƒå¤§çš„å­¦ä¹ ç‡

**è¾“å‡º**ï¼š
- `outputs/pretrain_projector/mm_projector.bin`ï¼šæŠ•å½±å±‚æƒé‡

### é˜¶æ®µ2ï¼šLoRAå¾®è°ƒ

å¾®è°ƒQwen3ï¼ˆä½¿ç”¨LoRAï¼‰+ æŠ•å½±å±‚ï¼ŒPatchTSTä»å†»ç»“ã€‚

```bash
# ä¿®æ”¹scripts/finetune_lora_multimodal.shä¸­çš„æ•°æ®è·¯å¾„
bash scripts/finetune_lora_multimodal.sh
```

**å…³é”®å‚æ•°**ï¼š
- `--pretrain_mm_mlp_adapter outputs/pretrain_projector/mm_projector.bin`ï¼šåŠ è½½é¢„è®­ç»ƒçš„æŠ•å½±å±‚
- `--lora_enable True`ï¼šå¯ç”¨LoRA
- `--lora_r 128`ï¼šLoRAç§©
- `--learning_rate 2e-4`ï¼šè¾ƒå°çš„å­¦ä¹ ç‡

**è¾“å‡º**ï¼š
- `outputs/finetune_lora/adapter_model.bin`ï¼šLoRAæƒé‡
- `outputs/finetune_lora/non_lora_trainables.bin`ï¼šæŠ•å½±å±‚æƒé‡

### é˜¶æ®µ2ï¼ˆå¯é€‰ï¼‰ï¼šQLoRAå¾®è°ƒï¼ˆ4-bité‡åŒ–ï¼Œå•GPUï¼‰

```bash
bash scripts/finetune_qlora_multimodal.sh
```

**ä¼˜åŠ¿**ï¼š
- æ˜¾å­˜éœ€æ±‚æ›´ä½ï¼ˆçº¦10GBå¯è®­ç»ƒ8Bæ¨¡å‹ï¼‰
- é€‚åˆå•GPUç¯å¢ƒ

## ğŸ” æµ‹è¯•éªŒè¯

### 1. æµ‹è¯•æ•°æ®åŠ è½½

```bash
python tools/test_multimodal_data.py \
    --data_path data/finetune.jsonl \
    --model_path Qwen/Qwen3-8B \
    --context_window 256 \
    --test all
```

**æµ‹è¯•å†…å®¹**ï¼š
- âœ… æ•°æ®é›†åŠ è½½ï¼ˆJSONLè§£æï¼Œ`<ts>`éªŒè¯ï¼‰
- âœ… DataCollatorï¼ˆæ‰¹å¤„ç†ï¼Œpaddingï¼‰
- âœ… æ¨¡å‹forwardï¼ˆlossè®¡ç®—ï¼‰

### 2. æ¨ç†ç¤ºä¾‹

```bash
# ä½¿ç”¨ç¤ºä¾‹æ•°æ®
python tools/inference_demo.py \
    --model_path Qwen/Qwen3-8B \
    --checkpoint outputs/finetune_lora \
    --max_new_tokens 512

# ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®
python tools/inference_demo.py \
    --model_path Qwen/Qwen3-8B \
    --checkpoint outputs/finetune_lora \
    --input_file data/test_sample.jsonl
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
qwen3_finetune/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ constants.py              # å…¨å±€å¸¸é‡ï¼ˆIGNORE_INDEX, TS_TOKEN_INDEXç­‰ï¼‰
â”‚   â”œâ”€â”€ dataset.py                # çº¯æ–‡æœ¬æ•°æ®é›†ï¼ˆåŸæœ‰ï¼‰
â”‚   â”œâ”€â”€ dataset_multimodal.py     # å¤šæ¨¡æ€æ•°æ®é›†ï¼ˆæ–°å¢ï¼‰
â”‚   â”œâ”€â”€ train.py                  # çº¯æ–‡æœ¬è®­ç»ƒï¼ˆåŸæœ‰ï¼‰
â”‚   â”œâ”€â”€ train_multimodal.py       # å¤šæ¨¡æ€è®­ç»ƒï¼ˆæ–°å¢ï¼‰
â”‚   â””â”€â”€ model/
â”‚       â”œâ”€â”€ projector.py          # MLPæŠ•å½±å±‚
â”‚       â”œâ”€â”€ ts_encoder.py         # PatchTSTç¼–ç å™¨åŒ…è£…
â”‚       â””â”€â”€ qwen3_ts.py           # å¤šæ¨¡æ€Qwen3æ¨¡å‹
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ pretrain_projector.sh     # é¢„è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ finetune_lora_multimodal.sh  # LoRAå¾®è°ƒè„šæœ¬
â”‚   â””â”€â”€ finetune_qlora_multimodal.sh # QLoRAå¾®è°ƒè„šæœ¬
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ test_multimodal_data.py   # æ•°æ®åŠ è½½æµ‹è¯•
â”‚   â””â”€â”€ inference_demo.py         # æ¨ç†ç¤ºä¾‹
â”œâ”€â”€ llava_model/                  # LLaVAå‚è€ƒä»£ç ï¼ˆä¸ä¿®æ”¹ï¼‰
â”œâ”€â”€ PatchTST_supervised/          # PatchTSTæºç å’Œæƒé‡
â”œâ”€â”€ configs/                      # DeepSpeedé…ç½®
â””â”€â”€ data/                         # æ•°æ®ç›®å½•
```

## ğŸ¯ ä½¿ç”¨æŠ€å·§

### 1. PatchTSTé…ç½®

å…³é”®è¶…å‚æ•°ï¼ˆåº”ä¸é¢„è®­ç»ƒPatchTSTä¿æŒä¸€è‡´ï¼‰ï¼š
- `--context_window 256`ï¼šæ—¶åºçª—å£é•¿åº¦
- `--patch_len 16`ï¼špatché•¿åº¦
- `--stride 8`ï¼špatchæ­¥é•¿
- `--ts_d_model 128`ï¼šPatchTSTè¾“å‡ºç»´åº¦

### 2. å†…å­˜ä¼˜åŒ–

å¦‚æœæ˜¾å­˜ä¸è¶³ï¼š
1. ä½¿ç”¨QLoRAï¼ˆ4-bité‡åŒ–ï¼‰
2. å‡å°batch sizeï¼Œå¢å¤§gradient accumulation
3. ä½¿ç”¨DeepSpeed Zero3
4. å‡å°`--model_max_length`

### 3. è®­ç»ƒç›‘æ§

ä½¿ç”¨wandbç›‘æ§è®­ç»ƒï¼š
```bash
# è®¾ç½®wandb
export WANDB_PROJECT="qwen3_ts"
export WANDB_API_KEY="your_key"
```

### 4. è°ƒè¯•æ¨¡å¼

å¿«é€ŸéªŒè¯æµç¨‹ï¼ˆä½¿ç”¨å°æ•°æ®é›†ï¼‰ï¼š
```bash
python src/train_multimodal.py \
    --model_name_or_path Qwen/Qwen3-8B \
    --data_path data/debug_10samples.jsonl \
    --output_dir outputs/debug \
    --num_train_epochs 1 \
    --save_steps 10 \
    --logging_steps 1
```

## ğŸ› å¸¸è§é—®é¢˜

### 1. `<ts>` tokenæ•°é‡ä¸åŒ¹é…

**é”™è¯¯**ï¼š`ValueError: æ–‡æœ¬ä¸­<ts>æ•°é‡(3)ä¸æ—¶é—´åºåˆ—å˜é‡æ•°(2)ä¸åŒ¹é…`

**è§£å†³**ï¼šæ£€æŸ¥JSONLæ•°æ®ï¼Œç¡®ä¿æ¯ä¸ªæ ·æœ¬çš„`<ts></ts>`æ•°é‡ç­‰äº`time_series`æ•°ç»„é•¿åº¦

### 2. PatchTSTæƒé‡åŠ è½½å¤±è´¥

**é”™è¯¯**ï¼š`FileNotFoundError: checkpoint.pthä¸å­˜åœ¨`

**è§£å†³**ï¼šæ£€æŸ¥`PatchTST_supervised/checkpoints/*/*/checkpoint.pth`è·¯å¾„

### 3. CUDA out of memory

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ä½¿ç”¨QLoRAï¼ˆ`--bits 4`ï¼‰
- å‡å°batch sizeï¼ˆ`--per_device_train_batch_size 1`ï¼‰
- å¢å¤§gradient accumulationï¼ˆ`--gradient_accumulation_steps 16`ï¼‰
- ä½¿ç”¨DeepSpeed Zero3ï¼ˆ`--deepspeed configs/zero3.json`ï¼‰

### 4. ä¸åŒé•¿åº¦çš„æ—¶é—´åºåˆ—

**è¯´æ˜**ï¼š
- âœ… åŒä¸€æ ·æœ¬å†…æ‰€æœ‰å˜é‡é•¿åº¦å¿…é¡»ç›¸åŒ
- âœ… ä¸åŒæ ·æœ¬çš„é•¿åº¦å¯ä»¥ä¸åŒï¼ˆä¼šè‡ªåŠ¨padding/æˆªæ–­åˆ°`context_window`ï¼‰
- âœ… ä¸åŒæ ·æœ¬çš„å˜é‡æ•°å¯ä»¥ä¸åŒï¼ˆé€æ ·æœ¬ç¼–ç ï¼‰

## ğŸ“ å¼•ç”¨

å¦‚æœä½¿ç”¨æœ¬é¡¹ç›®ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@software{qwen3_timeseries_multimodal,
  title = {Multimodal Time Series Qwen3 Model},
  author = {Your Name},
  year = {2025},
  note = {Building on LLaVA and PatchTST architectures}
}
```

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªåŸQwen3å’ŒLLaVAçš„è®¸å¯è¯ã€‚

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

---

**æœ€åæ›´æ–°**ï¼š2025-12-05
