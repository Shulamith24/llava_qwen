"""
æµ‹è¯•æ—¶åºç¼–ç å™¨çš„ä¿®å¤
éªŒè¯encoderèƒ½å¤Ÿå¤„ç†ä¸åŒå˜é‡æ•°çš„è¾“å…¥
"""

import sys
import torch
from pathlib import Path

# æ·»åŠ srcè·¯å¾„
current_file = Path(__file__).resolve()
parent_dir = current_file.parents[1]
sys.path.insert(0, str(parent_dir))

from src.model.ts_encoder import PatchTSTEncoderWrapper


def test_encoder_with_different_nvars():
    """æµ‹è¯•encoderå¤„ç†ä¸åŒå˜é‡æ•°çš„èƒ½åŠ›"""
    print("\n" + "="*60)
    print("æµ‹è¯•ï¼šEncoderå¤„ç†ä¸åŒå˜é‡æ•°")
    print("="*60)
    
    # åˆ›å»ºencoder
    context_window = 256
    patch_len = 16
    stride = 8
    d_model = 128
    
    encoder = PatchTSTEncoderWrapper(
        context_window=context_window,
        patch_len=patch_len,
        stride=stride,
        d_model=d_model,
        n_layers=3,
        n_heads=16,
        d_ff=256,
        dropout=0.1
    )
    
    print(f"âœ“ Encoderåˆ›å»ºæˆåŠŸ")
    print(f"  - context_window: {context_window}")
    print(f"  - patch_len: {patch_len}")
    print(f"  - stride: {stride}")
    print(f"  - d_model: {d_model}")
    print(f"  - expected patch_num: {encoder.patch_num}")
    
    # æµ‹è¯•ä¸åŒçš„å˜é‡æ•°
    test_cases = [
        (2, "2ä¸ªå˜é‡"),
        (5, "5ä¸ªå˜é‡"),
        (10, "10ä¸ªå˜é‡"),
    ]
    
    for n_vars, desc in test_cases:
        print(f"\næµ‹è¯•: {desc}")
        
        # åˆ›å»ºéšæœºè¾“å…¥
        x = torch.randn(n_vars, context_window)
        print(f"  è¾“å…¥shape: {x.shape}")
        
        # Forward
        with torch.no_grad():
            features = encoder(x)
        
        print(f"  è¾“å‡ºshape: {features.shape}")
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        expected_shape = (n_vars, encoder.patch_num, d_model)
        assert features.shape == expected_shape, \
            f"è¾“å‡ºshapeä¸åŒ¹é…ï¼æœŸæœ›{expected_shape}, å®é™…{features.shape}"
        
        print(f"  âœ“ è¾“å‡ºshapeæ­£ç¡®: {features.shape}")
    
    print("\n" + "="*60)
    print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    print("="*60)


def test_encoder_shared_parameters():
    """éªŒè¯ä¸åŒnvarsçš„æ ·æœ¬å…±äº«ç›¸åŒçš„encoderå‚æ•°"""
    print("\n" + "="*60)
    print("æµ‹è¯•ï¼šéªŒè¯å‚æ•°å…±äº«")
    print("="*60)
    
    encoder = PatchTSTEncoderWrapper(
        context_window=256,
        patch_len=16,
        stride=8,
        d_model=128,
    )
    
    # è®°å½•åˆå§‹å‚æ•°
    initial_params = {name: param.clone() for name, param in encoder.named_parameters()}
    
    # å¤„ç†ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼ˆ3ä¸ªå˜é‡ï¼‰
    x1 = torch.randn(3, 256)
    with torch.no_grad():
        _ = encoder(x1)
    
    # å¤„ç†ç¬¬äºŒä¸ªæ ·æœ¬ï¼ˆ7ä¸ªå˜é‡ï¼‰
    x2 = torch.randn(7, 256)
    with torch.no_grad():
        _ = encoder(x2)
    
    # éªŒè¯å‚æ•°æ²¡æœ‰å˜åŒ–ï¼ˆå› ä¸ºæ˜¯æ¨ç†æ¨¡å¼ï¼‰
    for name, param in encoder.named_parameters():
        assert torch.all(param == initial_params[name]), \
            f"å‚æ•°{name}å‘ç”Ÿäº†å˜åŒ–ï¼"
    
    print("âœ“ éªŒè¯é€šè¿‡ï¼šä¸åŒnvarsçš„æ ·æœ¬å…±äº«ç›¸åŒçš„encoder")
    print("âœ“ å‚æ•°æ€»æ•°:", sum(p.numel() for p in encoder.parameters()))
    

if __name__ == "__main__":
    try:
        test_encoder_with_different_nvars()
        test_encoder_shared_parameters()
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Encoderä¿®å¤æˆåŠŸã€‚")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        exit(1)
