"""
æµ‹è¯•å¤šæ¨¡æ€æ•°æ®åŠ è½½
éªŒè¯æ•°æ®é›†ã€DataCollatorå’Œæ¨¡å‹forwardæ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import sys
import os
import torch
from transformers import AutoTokenizer

# æ·»åŠ srcè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from dataset_multimodal import MultimodalDataset, DataCollatorForMultimodalDataset
from model.qwen3_ts import Qwen3TSConfig, Qwen3TSForCausalLM
from constants import DEFAULT_TS_TOKEN, TS_TOKEN_INDEX
import constants as GLOBAL_CONSTANTS


def test_dataset_loading(data_path, tokenizer, context_window=256):
    """æµ‹è¯•æ•°æ®é›†åŠ è½½"""
    print("\n" + "="*60)
    print("æµ‹è¯•1ï¼šæ•°æ®é›†åŠ è½½")
    print("="*60)
    
    try:
        dataset = MultimodalDataset(
            data_path=data_path,
            tokenizer=tokenizer,
            model_max_length=2048,
            context_window=context_window
        )
        
        print(f"âœ“ æˆåŠŸåŠ è½½æ•°æ®é›†ï¼Œå…± {len(dataset)} æ¡æ ·æœ¬")
        
        # æµ‹è¯•ç¬¬ä¸€ä¸ªæ ·æœ¬
        sample = dataset[0]
        print(f"\næ ·æœ¬ 0:")
        print(f"  - input_ids shape: {sample['input_ids'].shape}")
        print(f"  - labels shape: {sample['labels'].shape}")
        print(f"  - time_series shape: {sample['time_series'].shape}")
        print(f"  - time_series: [n_vars={sample['time_series'].shape[0]}, seq_len={sample['time_series'].shape[1]}]")
        
        # è§£ç æ–‡æœ¬
        text = tokenizer.decode(sample['input_ids'], skip_special_tokens=False)
        print(f"\n  æ–‡æœ¬å†…å®¹ï¼ˆå‰200å­—ç¬¦ï¼‰:\n  {text[:200]}...")
        
        # ç»Ÿè®¡<ts> tokenæ•°é‡
        ts_count = (sample['input_ids'] == TS_TOKEN_INDEX).sum().item()
        print(f"\n  <ts> tokenæ•°é‡: {ts_count}")
        print(f"  æ—¶åºå˜é‡æ•°: {sample['time_series'].shape[0]}")
        
        if ts_count == sample['time_series'].shape[0]:
            print(f"  âœ“ <ts>æ•°é‡ä¸å˜é‡æ•°ä¸€è‡´")
        else:
            print(f"  âœ— è­¦å‘Šï¼š<ts>æ•°é‡ä¸å˜é‡æ•°ä¸ä¸€è‡´ï¼")
        
        return True
    
    except Exception as e:
        print(f"âœ— æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_data_collator(data_path, tokenizer, context_window=256):
    """æµ‹è¯•DataCollator"""
    print("\n" + "="*60)
    print("æµ‹è¯•2ï¼šDataCollatorï¼ˆæ‰¹å¤„ç†ï¼‰")
    print("="*60)
    
    try:
        dataset = MultimodalDataset(
            data_path=data_path,
            tokenizer=tokenizer,
            model_max_length=2048,
            context_window=context_window
        )
        
        collator = DataCollatorForMultimodalDataset(tokenizer=tokenizer)
        
        # åˆ›å»ºä¸€ä¸ªbatchï¼ˆ2ä¸ªæ ·æœ¬ï¼‰
        batch_size = min(2, len(dataset))
        instances = [dataset[i] for i in range(batch_size)]
        
        batch = collator(instances)
        
        print(f"âœ“ æˆåŠŸåˆ›å»ºbatchï¼Œbatch_size={batch_size}")
        print(f"\nBatchå†…å®¹:")
        print(f"  - input_ids: {batch['input_ids'].shape}")
        print(f"  - labels: {batch['labels'].shape}")
        print(f"  - attention_mask: {batch['attention_mask'].shape}")
        print(f"  - time_series: List[Tensor], é•¿åº¦={len(batch['time_series'])}")
        
        for i, ts in enumerate(batch['time_series']):
            print(f"    - æ ·æœ¬{i}: {ts.shape}")
        
        return True
    
    except Exception as e:
        print(f"âœ— DataCollatoræµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_model_forward(data_path, model_path, context_window=256):
    """æµ‹è¯•æ¨¡å‹forward"""
    print("\n" + "="*60)
    print("æµ‹è¯•3ï¼šæ¨¡å‹forward")
    print("="*60)
    
    try:
        # åŠ è½½tokenizer
        print(f"åŠ è½½tokenizer: {model_path}")
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            model_max_length=2048,
            padding_side="right",
            use_fast=False,
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # æ·»åŠ <ts> token
        print(f"æ·»åŠ ç‰¹æ®Štoken: {DEFAULT_TS_TOKEN}")
        num_new_tokens = tokenizer.add_tokens([DEFAULT_TS_TOKEN], special_tokens=True)
        GLOBAL_CONSTANTS.TS_TOKEN_INDEX = tokenizer.convert_tokens_to_ids(DEFAULT_TS_TOKEN)
        print(f"  TS_TOKEN_INDEX = {GLOBAL_CONSTANTS.TS_TOKEN_INDEX}")
        
        # åˆ›å»ºé…ç½®
        print(f"\nåˆ›å»ºæ¨¡å‹é…ç½®...")
        config = Qwen3TSConfig.from_pretrained(
            model_path,
            mm_ts_tower="patchtst",
            patchtst_checkpoint=None,  # æµ‹è¯•ä¸åŠ è½½æƒé‡
            freeze_patchtst=True,
            context_window=context_window,
            patch_len=16,
            stride=8,
            ts_d_model=128,
            ts_n_layers=3,
            ts_n_heads=16,
            ts_d_ff=256,
            mm_projector_type="mlp2x_gelu",
        )
        
        # åŠ è½½æ¨¡å‹
        print(f"åŠ è½½æ¨¡å‹: {model_path}")
        model = Qwen3TSForCausalLM.from_pretrained(
            model_path,
            config=config,
            torch_dtype=torch.float16,
        )
        
        # Resize embedding
        if num_new_tokens > 0:
            model.resize_token_embeddings(len(tokenizer))
        
        model.eval()
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.to(device)
        
        print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {device}")
        
        # å‡†å¤‡æ•°æ®
        dataset = MultimodalDataset(
            data_path=data_path,
            tokenizer=tokenizer,
            model_max_length=2048,
            context_window=context_window
        )
        
        collator = DataCollatorForMultimodalDataset(tokenizer=tokenizer)
        batch = collator([dataset[0]])
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        time_series = [ts.to(device) for ts in batch['time_series']]
        
        print(f"\næ‰§è¡Œforward...")
        with torch.no_grad():
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels,
                time_series=time_series
            )
        
        print(f"âœ“ ForwardæˆåŠŸï¼")
        print(f"  - loss: {outputs.loss.item():.4f}")
        print(f"  - logits shape: {outputs.logits.shape}")
        
        return True
    
    except Exception as e:
        print(f"âœ— æ¨¡å‹forwardå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="æµ‹è¯•å¤šæ¨¡æ€æ•°æ®åŠ è½½å’Œæ¨¡å‹")
    parser.add_argument("--data_path", type=str, required=True, help="JSONLæ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model_path", type=str, default="Qwen/Qwen3-8B", help="Qwen3æ¨¡å‹è·¯å¾„")
    parser.add_argument("--context_window", type=int, default=256, help="æ—¶åºçª—å£é•¿åº¦")
    parser.add_argument("--test", type=str, default="all", choices=["all", "dataset", "collator", "model"],
                       help="æµ‹è¯•ç±»å‹")
    
    args = parser.parse_args()
    
    print("\n" + "="*60)
    print("å¤šæ¨¡æ€æ•°æ®åŠ è½½æµ‹è¯•")
    print("="*60)
    print(f"æ•°æ®è·¯å¾„: {args.data_path}")
    print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"æ—¶åºçª—å£: {args.context_window}")
    
    # åŠ è½½tokenizerï¼ˆç”¨äºæµ‹è¯•1å’Œ2ï¼‰
    if args.test in ["all", "dataset", "collator"]:
        print(f"\nåŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            args.model_path,
            model_max_length=2048,
            padding_side="right",
            use_fast=False,
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # æ·»åŠ <ts> token
        num_new_tokens = tokenizer.add_tokens([DEFAULT_TS_TOKEN], special_tokens=True)
        GLOBAL_CONSTANTS.TS_TOKEN_INDEX = tokenizer.convert_tokens_to_ids(DEFAULT_TS_TOKEN)
        print(f"âœ“ TokenizeråŠ è½½å®Œæˆ")
        print(f"  æ·»åŠ  {num_new_tokens} ä¸ªæ–°token")
        print(f"  TS_TOKEN_INDEX = {GLOBAL_CONSTANTS.TS_TOKEN_INDEX}")
    
    # è¿è¡Œæµ‹è¯•
    results = {}
    
    if args.test in ["all", "dataset"]:
        results["dataset"] = test_dataset_loading(args.data_path, tokenizer, args.context_window)
    
    if args.test in ["all", "collator"]:
        results["collator"] = test_data_collator(args.data_path, tokenizer, args.context_window)
    
    if args.test in ["all", "model"]:
        results["model"] = test_model_forward(args.data_path, args.model_path, args.context_window)
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("æµ‹è¯•æ€»ç»“")
    print("="*60)
    for test_name, result in results.items():
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        print(f"  {test_name}: {status}")
    
    all_passed = all(results.values())
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return 0
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        return 1


if __name__ == "__main__":
    exit(main())
